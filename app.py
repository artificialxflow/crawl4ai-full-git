import sys
import asyncio

if sys.platform.startswith("win"):
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

import nest_asyncio
nest_asyncio.apply()

import streamlit as st
import requests
from bs4 import BeautifulSoup
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig
from docx import Document
from fpdf import FPDF
import tempfile
import re
from dotenv import load_dotenv
import os
import json

# Load OpenRouter API key
load_dotenv('.env.local')
OPENROUTER_API_KEY = os.getenv('OPENROUTER_API_KEY')
OPENROUTER_MODEL = 'openai/gpt-4o'  # You can change this to your preferred model

# Inject custom CSS for RTL support and centering
st.markdown(
    """
    <style>
    body, .stApp {
        direction: rtl;
        text-align: right;
        font-family: Tahoma, Arial, sans-serif;
        background: #f9f9f9;
    }
    .centered-input {
        display: flex;
        justify-content: center;
        align-items: center;
        margin-bottom: 1.5rem;
    }
    .stTextInput > div > input {
        text-align: right;
    }
    .stDownloadButton > button {
        direction: rtl;
    }
    </style>
    """,
    unsafe_allow_html=True
)

st.title("ğŸ•¸ï¸ Ø³Ø§ÛŒØªâ€ŒÙ…Ù¾ Ú©Ø±Ø§Ù„Ø± (RTL Sitemap Crawler)")

# State for parsed sitemap and crawl results
if 'sitemap_data' not in st.session_state:
    st.session_state['sitemap_data'] = None
if 'sitemap_error' not in st.session_state:
    st.session_state['sitemap_error'] = None
if 'crawl_results' not in st.session_state:
    st.session_state['crawl_results'] = None
if 'crawl_status' not in st.session_state:
    st.session_state['crawl_status'] = None
if 'is_crawling' not in st.session_state:
    st.session_state['is_crawling'] = False
if 'explanation' not in st.session_state:
    st.session_state['explanation'] = None
if 'knowledge_base' not in st.session_state:
    st.session_state['knowledge_base'] = None
if 'qa_answer' not in st.session_state:
    st.session_state['qa_answer'] = None
if 'archive' not in st.session_state:
    st.session_state['archive'] = []

# Sidebar: Archive of crawled sitemaps
st.sidebar.title('Ø¢Ø±Ø´ÛŒÙˆ Ø³Ø§ÛŒØªâ€ŒÙ…Ù¾â€ŒÙ‡Ø§')
if st.session_state['archive']:
    for i, entry in enumerate(st.session_state['archive']):
        st.sidebar.markdown(f"{i+1}. [{entry['url']}]({entry['url']})")
else:
    st.sidebar.info('Ù‡Ù†ÙˆØ² Ù‡ÛŒÚ† Ø³Ø§ÛŒØªâ€ŒÙ…Ù¾ÛŒ Ø®Ø²ÛŒØ¯Ù‡ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª.')

# Centered input for sitemap URL (empty by default)
with st.form(key="sitemap_form"):
    st.markdown('<div class="centered-input">', unsafe_allow_html=True)
    sitemap_url = st.text_input("Ø¢Ø¯Ø±Ø³ Ø³Ø§ÛŒØªâ€ŒÙ…Ù¾ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯:", key="sitemap_url", value="")
    st.markdown('</div>', unsafe_allow_html=True)
    submit = st.form_submit_button("Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø§ÛŒØªâ€ŒÙ…Ù¾")

    if submit:
        st.session_state['sitemap_data'] = None
        st.session_state['sitemap_error'] = None
        st.session_state['crawl_results'] = None
        st.session_state['crawl_status'] = None
        st.session_state['is_crawling'] = False
        st.session_state['explanation'] = None
        st.session_state['knowledge_base'] = None
        st.session_state['qa_answer'] = None
        # Add to archive if not already present
        if sitemap_url and not any(entry['url'] == sitemap_url for entry in st.session_state['archive']):
            st.session_state['archive'].append({'url': sitemap_url})
        try:
            resp = requests.get(sitemap_url, timeout=10)
            resp.raise_for_status()
            soup = BeautifulSoup(resp.content, 'xml')
            urls = []
            for url in soup.find_all('url'):
                loc = url.find('loc').text if url.find('loc') else ''
                lastmod = url.find('lastmod').text if url.find('lastmod') else ''
                priority = url.find('priority').text if url.find('priority') else ''
                urls.append({'loc': loc, 'lastmod': lastmod, 'priority': priority})
            if urls:
                st.session_state['sitemap_data'] = urls
            else:
                st.session_state['sitemap_error'] = "Ù‡ÛŒÚ† URL Ù…Ø¹ØªØ¨Ø±ÛŒ Ø¯Ø± Ø³Ø§ÛŒØªâ€ŒÙ…Ù¾ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯."
        except Exception as e:
            st.session_state['sitemap_error'] = f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª ÛŒØ§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø³Ø§ÛŒØªâ€ŒÙ…Ù¾: {e}"

# Enable question input if sitemap is parsed and explanation is ready
question_disabled = not (st.session_state.get('sitemap_data') and not st.session_state.get('sitemap_error') and st.session_state.get('explanation'))

# Add a submit button for the question input
with st.form(key="qa_form"):
    st.markdown('<div class="centered-input">', unsafe_allow_html=True)
    question = st.text_input("Ø³ÙˆØ§Ù„ Ø®ÙˆØ¯ Ø±Ø§ Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ø´Ø±Ú©Øª ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯:", key="question", disabled=question_disabled)
    st.markdown('</div>', unsafe_allow_html=True)
    qa_submit = st.form_submit_button("Ù¾Ø±Ø³Ø´ Ø³ÙˆØ§Ù„")

# Start crawling button
if st.session_state.get('sitemap_data'):
    if not st.session_state['is_crawling'] and not st.session_state.get('crawl_results'):
        if st.button('Ø´Ø±ÙˆØ¹ Ø®Ø²ÛŒØ¯Ù†'):
            st.session_state['is_crawling'] = True
            st.rerun()

# Async crawl logic
async def crawl_all(urls):
    browser_conf = BrowserConfig(browser_type="firefox", headless=True, text_mode=True)
    crawl_status = {}
    crawl_results = {}
    async with AsyncWebCrawler(config=browser_conf) as crawler:
        results = await crawler.arun_many(
            urls=[u['loc'] for u in urls],
            config=CrawlerRunConfig()
        )
        for res in results:
            if res.success:
                crawl_status[res.url] = 'success'
                crawl_results[res.url] = res.markdown
            else:
                crawl_status[res.url] = 'error'
                crawl_results[res.url] = res.error_message
    return crawl_status, crawl_results

# Run crawling if triggered
if st.session_state.get('is_crawling') and st.session_state.get('sitemap_data') and not st.session_state.get('crawl_results'):
    with st.spinner('Ø¯Ø± Ø­Ø§Ù„ Ø®Ø²ÛŒØ¯Ù† ØµÙØ­Ø§Øª...'):
        loop = asyncio.get_event_loop()
        crawl_status, crawl_results = loop.run_until_complete(crawl_all(st.session_state['sitemap_data']))
        st.session_state['crawl_status'] = crawl_status
        st.session_state['crawl_results'] = crawl_results
        st.session_state['is_crawling'] = False
        st.rerun()

# --- LLM Content Analysis & Explanation Generation ---
def call_openrouter_llm(prompt, model=OPENROUTER_MODEL, max_tokens=1024, temperature=0.2):
    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json"
    }
    data = {
        "model": model,
        "messages": [
            {"role": "system", "content": "Ø´Ù…Ø§ ÛŒÚ© ØªØ­Ù„ÛŒÙ„â€ŒÚ¯Ø± Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ ÙˆØ¨â€ŒØ³Ø§ÛŒØª Ù‡Ø³ØªÛŒØ¯."},
            {"role": "user", "content": prompt}
        ],
        "max_tokens": max_tokens,
        "temperature": temperature
    }
    response = requests.post("https://openrouter.ai/api/v1/chat/completions", headers=headers, data=json.dumps(data))
    try:
        resp_json = response.json()
    except Exception:
        return f"[Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ù¾Ø§Ø³Ø® Ø§Ø² LLM: {response.text}]"
    if response.status_code == 200 and 'choices' in resp_json:
        return resp_json['choices'][0]['message']['content']
    elif 'error' in resp_json:
        return f"[Ø®Ø·Ø§ Ø§Ø² LLM: {resp_json['error'].get('message', str(resp_json['error']))}]"
    else:
        return f"[Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ù¾Ø§Ø³Ø® Ø§Ø² LLM: {resp_json}]"

def build_analysis_prompt(all_text):
    return f"""
    Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ù…Ø­ØªÙˆØ§ÛŒ Ø²ÛŒØ± Ú©Ù‡ Ø§Ø² ØµÙØ­Ø§Øª Ù…Ø®ØªÙ„Ù ÛŒÚ© Ø´Ø±Ú©Øª Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡ Ø§Ø³ØªØŒ Ù„Ø·ÙØ§Ù‹ Ù…ÙˆØ§Ø±Ø¯ Ø²ÛŒØ± Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¯Ù‚ÛŒÙ‚ Ùˆ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ùˆ Ø¨Ø§ Ù„Ø­Ù† Ø§Ù†Ø³Ø§Ù†ÛŒ Ùˆ Ù‚Ø§Ø¨Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²Ø§Ø±ÛŒØ§Ø¨ÛŒ Ùˆ Ù…Ø¹Ø±ÙÛŒ Ø´Ø±Ú©Øª Ø§Ø±Ø§Ø¦Ù‡ Ú©Ù†:
    1. Ø®Ù„Ø§ØµÙ‡â€ŒØ§ÛŒ Ú©Ø§Ù…Ù„ Ùˆ Ù‚Ø§Ø¨Ù„ ÙÙ‡Ù… Ø§Ø² Ø´Ø±Ú©Øª Ùˆ ÙØ¹Ø§Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø¢Ù†
    2. Ù…Ø­ØµÙˆÙ„Ø§Øª Ø§ØµÙ„ÛŒ Ø´Ø±Ú©Øª Ùˆ ØªÙˆØ¶ÛŒØ­ Ù‡Ø±Ú©Ø¯Ø§Ù…
    3. Ø®Ø¯Ù…Ø§Øª Ø§ØµÙ„ÛŒ Ø´Ø±Ú©Øª Ùˆ ØªÙˆØ¶ÛŒØ­ Ù‡Ø±Ú©Ø¯Ø§Ù…
    4. ØªØ®ØµØµâ€ŒÙ‡Ø§ Ùˆ Ù…Ø²ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø±Ù‚Ø§Ø¨ØªÛŒ Ø´Ø±Ú©Øª
    5. Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ù…Ù‡Ù… Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ù…Ø­ØµÙˆÙ„Ø§Øª Ùˆ Ø®Ø¯Ù…Ø§Øª
    6. Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ù…Ø´ØªØ±ÛŒØ§Ù† Ø¨Ø§Ù„Ù‚ÙˆÙ‡
    7. ÛŒÚ© Ø§ÛŒÙ…ÛŒÙ„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ Ù…Ø´ØªØ±ÛŒØ§Ù† Ø¨Ø§Ù„Ù‚ÙˆÙ‡ Ø¬Ù‡Øª Ù…Ø¹Ø±ÙÛŒ Ø´Ø±Ú©Øª Ùˆ Ù…Ø­ØµÙˆÙ„Ø§Øª/Ø®Ø¯Ù…Ø§Øª
    8. Ù‡Ø± Ù†Ú©ØªÙ‡ Ù…Ù‡Ù… Ø¯ÛŒÚ¯Ø±ÛŒ Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ø´Ù†Ø§Ø®Øª Ø´Ø±Ú©Øª Ù…ÙÛŒØ¯ Ø§Ø³Øª
    Ù…Ø­ØªÙˆØ§ÛŒ ØµÙØ­Ø§Øª:
    ---
    {all_text}
    ---
    Ø®Ø±ÙˆØ¬ÛŒ Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ù…Ù†Ø¸Ù… Ùˆ Ù‚Ø§Ø¨Ù„ Ø®ÙˆØ§Ù†Ø¯Ù† Ùˆ Ø¨Ø§ ØªÛŒØªØ±Ù‡Ø§ÛŒ Ù…Ø´Ø®Øµ Ø§Ø±Ø§Ø¦Ù‡ Ú©Ù†."
    """

def build_qa_prompt(all_text, question):
    return f"""
    Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø²ÛŒØ± Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ø´Ø±Ú©ØªØŒ Ø¨Ù‡ Ø³ÙˆØ§Ù„ Ø²ÛŒØ± Ø¨Ù‡ ØµÙˆØ±Øª Ø¯Ù‚ÛŒÙ‚ Ùˆ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ùˆ Ø¨Ø§ Ù„Ø­Ù† Ø§Ù†Ø³Ø§Ù†ÛŒ Ù¾Ø§Ø³Ø® Ø¨Ø¯Ù‡:
    Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø´Ø±Ú©Øª:
    ---
    {all_text}
    ---
    Ø³ÙˆØ§Ù„:
    {question}
    Ù¾Ø§Ø³Ø® Ø±Ø§ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ùˆ Ø¨Ø§ Ø¬Ø²Ø¦ÛŒØ§Øª Ú©Ø§ÙÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ú©Ù†.
    """

# Generate explanation after crawling using LLM
if st.session_state.get('crawl_results') and not st.session_state.get('explanation'):
    all_text = '\n\n'.join([v for v in st.session_state['crawl_results'].values() if isinstance(v, str) and len(v) > 30])
    MAX_CHARS = 40000
    if len(all_text) > MAX_CHARS:
        all_text = all_text[:MAX_CHARS]
    with st.spinner('Ø¯Ø± Ø­Ø§Ù„ ØªØ­Ù„ÛŒÙ„ Ù…Ø­ØªÙˆØ§ÛŒ Ø³Ø§ÛŒØª Ø¨Ø§ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ...'):
        prompt = build_analysis_prompt(all_text)
        explanation = call_openrouter_llm(prompt)
    st.session_state['explanation'] = explanation
    st.session_state['knowledge_base'] = all_text  # For QA

# Show explanation and export buttons
if st.session_state.get('explanation'):
    st.markdown('<div style="background:#fff;padding:1.5rem;border-radius:8px;box-shadow:0 2px 8px #eee;">', unsafe_allow_html=True)
    st.markdown(st.session_state['explanation'], unsafe_allow_html=True)
    st.markdown('</div>', unsafe_allow_html=True)

    # Export to Word
    doc = Document()
    doc.add_heading('ØªÙˆØ¶ÛŒØ­ Ø´Ø±Ú©Øª Ùˆ Ù…Ø­ØµÙˆÙ„Ø§Øª', 0)
    doc.add_paragraph(re.sub('<[^<]+?>', '', st.session_state['explanation']))
    word_file = tempfile.NamedTemporaryFile(delete=False, suffix='.docx')
    doc.save(word_file.name)
    with open(word_file.name, 'rb') as f:
        st.download_button('Ø¯Ø§Ù†Ù„ÙˆØ¯ Word', f, file_name='company_explanation.docx')

    # Export to PDF
    pdf = FPDF()
    pdf.add_page()
    pdf.add_font('DejaVu', '', fname='C:/Windows/Fonts/arial.ttf', uni=True)  # Adjust font path as needed
    pdf.set_font('DejaVu', '', 14)
    for line in re.sub('<[^<]+?>', '', st.session_state['explanation']).split('\n'):
        pdf.cell(0, 10, line, ln=True, align='R')
    pdf_file = tempfile.NamedTemporaryFile(delete=False, suffix='.pdf')
    pdf.output(pdf_file.name)
    with open(pdf_file.name, 'rb') as f:
        st.download_button('Ø¯Ø§Ù†Ù„ÙˆØ¯ PDF', f, file_name='company_explanation.pdf')

# LLM-powered QA from knowledge base
if qa_submit and st.session_state.get('knowledge_base'):
    all_text = st.session_state['knowledge_base']
    MAX_CHARS = 40000
    if len(all_text) > MAX_CHARS:
        all_text = all_text[:MAX_CHARS]
    with st.spinner('Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø±ÛŒØ§ÙØª Ù¾Ø§Ø³Ø® Ø§Ø² Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ...'):
        qa_prompt = build_qa_prompt(all_text, question)
        answer = call_openrouter_llm(qa_prompt)
    st.session_state['qa_answer'] = answer

if st.session_state.get('qa_answer'):
    st.info(st.session_state['qa_answer']) 